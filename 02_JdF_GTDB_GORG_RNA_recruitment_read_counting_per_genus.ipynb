{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# October 27th 2022 update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a modification of the script that Julia originally wrote to summarize the GORG classifier outputs into read counts per gene per taxonomy. After extensive examination of the GORG classifier results and reading the Kaiju paper I decided that I can just use the \"taxonomy_ids\" which corresponds to the \"taxonomy_lineage\" column as my groupby column. The \"taxonomy_id\" is the highest classification with best matches. eg if it matches two species that are part of the same genus the taxonomy_ids is the genus those species belong to.\n",
    "\n",
    "In order to keep all hypothetical genes from being combined into a single \"hypothetical protein\" and also to ensure that slight cell to cell variations of the same gene were clustered the same I ran mmseqs on the same file that was used to generate the GORG database to cluster the proteins together. Then I used the gene_cluster_rep as the unique identifier for a cluster of genes. I used the first gene in the GORG \"sequence_ids\" file to identify the gene_cluster_rep sequence and then grouped the dataframe on the gene_cluster_rep column and the taxonomy_ids column. This produces a file with the number of reads assigned to each gene for each taxonomic lineage. Once I have this file I can parse it into a wide output (see the next notebook) and then proceed with differential analyses and any other analyses that I am intersted in doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All together this notebook takes ~2 hours to process ~1.124 billion RNA reads when run in a standard interactive job with 10Gb and 8 CPUs\n",
    "\n",
    "\n",
    "An average of ~10% of reads were recruiting to the genes. I need to process the output files to see what percent of these reads are trna or other ribosomal proteins but that should be pretty straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import os.path as op\n",
    "import math\n",
    "import os \n",
    "import sys\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "import sqlite3\n",
    "import sqlalchemy\n",
    "\n",
    "def safe_makedir(dname):\n",
    "    \"\"\"\n",
    "    Make a directory if it doesn't exist, handling concurrent race conditions.\n",
    "    \"\"\"\n",
    "    if not dname:\n",
    "        return dname\n",
    "    num_tries = 0\n",
    "    max_tries = 5\n",
    "    while not os.path.exists(dname):\n",
    "        try:\n",
    "            os.makedirs(dname)\n",
    "        except OSError:\n",
    "            if num_tries > max_tries:\n",
    "                raise\n",
    "            num_tries += 1\n",
    "            time.sleep(2)\n",
    "    return dname\n",
    "\n",
    "# import my custom functions most of these are ease of life functions and are not necessary for a lot of my notebooks\n",
    "sys.path.insert(0, '/home/jmunson-mcgee/')\n",
    "from JMM_functions import *\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 6)\n",
    "\n",
    "outdir='/mnt/scgc/simon/microg2p/Melody/BLM1/Analyses/RNA_GORG_recruit/Summary_files'\n",
    "safe_make_dir(outdir)\n",
    "os.chdir(outdir)\n",
    "\n",
    "    \n",
    "matplotlib.__version__\n",
    "\n",
    "read_files = glob.glob('/mnt/scgc/simon/microg2p/Melody/BLM1/Analyses/RNA_GORG_recruit/annotations/*.txt.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file that indicates which sags belong to which category\n",
    "\n",
    "cat_file = '/mnt/scgc/simon/microg2p/Melody/BLM1/Data/BLM1_Qc_MELODY_210625_VH00511_17_AAAG2WYHV_assembly_stats.csv'\n",
    "namecolumnid = 'well'\n",
    "categorycolumnid = 'GTDB_classification'\n",
    "\n",
    "#catdf_file = '/mnt/scgc/simon/microg2p/analyses/ani/All_GoM_SAGs_1cell_20kb_decon_clusters_added.csv'\n",
    "catdf = pd.read_csv(cat_file)\n",
    "\n",
    "\n",
    "gadict = {l[namecolumnid]:l[categorycolumnid] for i, l in catdf.iterrows()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE\n",
    "The mmseqs clustering has to be run on exactly the same file that was used to create the classifier database. If Greg/Ramunas want to continue to update the classifier and make it more user friendly then this is probably something that should be included before producing the output summary file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene_cluster_rep</th>\n",
       "      <th>gene</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AM-294-P06_N42;11716;12369</td>\n",
       "      <td>AM-294-P06_N42;11716;12369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AM-294-P06_N42;11716;12369</td>\n",
       "      <td>AM-307-N22_N5;46105;46758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AM-294-E20_N5;7770;9170</td>\n",
       "      <td>AM-294-E20_N5;7770;9170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124814</th>\n",
       "      <td>AM-294-L14_N3;8038;8610</td>\n",
       "      <td>AM-294-E11_N9;13611;14183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124815</th>\n",
       "      <td>AM-302-A21_N4;6358;7056</td>\n",
       "      <td>AM-302-A21_N4;6358;7056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124816</th>\n",
       "      <td>AM-302-A21_N4;6358;7056</td>\n",
       "      <td>AM-304-G09_N3;29860;30558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124817 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  gene_cluster_rep                        gene\n",
       "0       AM-294-P06_N42;11716;12369  AM-294-P06_N42;11716;12369\n",
       "1       AM-294-P06_N42;11716;12369   AM-307-N22_N5;46105;46758\n",
       "2          AM-294-E20_N5;7770;9170     AM-294-E20_N5;7770;9170\n",
       "...                            ...                         ...\n",
       "124814     AM-294-L14_N3;8038;8610   AM-294-E11_N9;13611;14183\n",
       "124815     AM-302-A21_N4;6358;7056     AM-302-A21_N4;6358;7056\n",
       "124816     AM-302-A21_N4;6358;7056   AM-304-G09_N3;29860;30558\n",
       "\n",
       "[124817 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmseqs_cluster_file = '/mnt/scgc/simon/microg2p/Melody/BLM1/Analyses/mmseqs2/BLM1_GORG_genes_60minid_m80.tsv'\n",
    "\n",
    "head = ['gene_cluster_rep', 'gene']\n",
    "gcdf = pd.read_csv(mmseqs_cluster_file,\n",
    "                         sep='\\t', names=head)\n",
    "# this gets rid of the tax_id data that is at the end of the gene_name and gene_cluster_rep strings. \n",
    "gcdf['gene_cluster_rep'] = [\"_\".join(i.split(\"_\")[:-1]) for i in gcdf['gene_cluster_rep']]\n",
    "gcdf['gene'] = [\"_\".join(i.split(\"_\")[:-1]) for i in gcdf['gene']]\n",
    "\n",
    "# for some reason the '-' were removed in the creation of the GORG database this time so I have to modify things to include this character\n",
    "gcdf['gene_cluster_rep']=gcdf['gene_cluster_rep'].str[:2] + '-' + gcdf['gene_cluster_rep'].str[2:5] + '-'+gcdf['gene_cluster_rep'].str[5:]\n",
    "gcdf['gene']=gcdf['gene'].str[:2] + '-' + gcdf['gene'].str[2:5] + '-'+gcdf['gene'].str[5:]\n",
    "\n",
    "gcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('shared', 69140), ('exclusive', 55677)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you should be able to get rid of this cell it is left over from a different way to identify if \n",
    "# reads were shared between tax lineages but the GORG tax lineage should take care of this\n",
    "gcdf['gene_sag'] = [i.split(\"_\")[0] for i in gcdf['gene']]\n",
    "gcdf['gene_genus'] = [gadict[i] for i in gcdf['gene_sag']]\n",
    "\n",
    "# this counts the number of genes represented in each gene cluster\n",
    "genus_per_cluster = gcdf[gcdf['gene_genus'] != 'Unclassified'].drop_duplicates(subset=['gene_cluster_rep','gene_genus']).groupby('gene_cluster_rep', as_index=False)['gene'].count().rename(columns={'gene':'genus_count'})\n",
    "\n",
    "genus_per_cluster['gene_cluster_genus_status'] = ['exclusive' if i == 1 else 'shared' for i in genus_per_cluster['genus_count']]\n",
    "gcdf2 = gcdf.merge(genus_per_cluster[['gene_cluster_rep','gene_cluster_genus_status']], how='left')\n",
    "\n",
    "gcdf2['gene_cluster_genus_status'] = gcdf2['gene_cluster_genus_status'].fillna('unclassified_only')\n",
    "\n",
    "\n",
    "\n",
    "Counter(gcdf2['gene_cluster_genus_status']).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes from 2022-10-26\n",
    "The next two cells are code that Jacob started writing  on Monday October 24th 2022 to parse the results. I spent a lot of time going through the Kaiju paper as well as the GORG output files and I have some thoughts. The taxid that is assigned in the 'taxonomy_id' and the 'taxonomic_lineage' are the highest taxa (eg the lca). If one cell has a higher classification ranking it is put in the 'taxonomy_ids_lca' oclumn. As a result of this all that I need to do is add the 'gene_cluster_rep' and then group the reads by the 'taxonomy_id' and 'gene_cluster_rep'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['status',\n",
       " 'sequence_id',\n",
       " 'taxonomy_id',\n",
       " 'length',\n",
       " 'taxonomy_ids_lca',\n",
       " 'sequence_ids_lca',\n",
       " 'protein_sequence',\n",
       " 'taxonomic_lineage',\n",
       " 'prokka_EC_number',\n",
       " 'prokka_product',\n",
       " 'swissprot_gene',\n",
       " 'swissprot_EC_number',\n",
       " 'swissprot_eggNOG',\n",
       " 'swissprot_KO',\n",
       " 'swissprot_Pfam',\n",
       " 'swissprot_CAZy',\n",
       " 'swissprot_TIGRFAMs']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I can just pick one of the output files for the header because they are all identical. This is because I had to chunk the files.\n",
    "names=pd.read_csv('/mnt/scgc/simon/microg2p/JdF_analysis/analysis/JdF_GORG_RNA_recruit/annotations/All_10thiosulfate_reads_noMTST_annotated.txt.gz',\n",
    "           compression='gzip', nrows=0, sep='\\t')\n",
    "header=names.columns.to_list()\n",
    "\n",
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene_cluster_rep</th>\n",
       "      <th>gene</th>\n",
       "      <th>gene_sag</th>\n",
       "      <th>gene_genus</th>\n",
       "      <th>gene_cluster_genus_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AM-294-P06_N42;11716;12369</td>\n",
       "      <td>AM-294-P06_N42;11716;12369</td>\n",
       "      <td>AM-294-P06</td>\n",
       "      <td>d__Bacteria;p__Desulfobacterota;c__BSN033;o__B...</td>\n",
       "      <td>exclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AM-294-P06_N42;11716;12369</td>\n",
       "      <td>AM-307-N22_N5;46105;46758</td>\n",
       "      <td>AM-307-N22</td>\n",
       "      <td>d__Bacteria;p__Desulfobacterota;c__BSN033;o__B...</td>\n",
       "      <td>exclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AM-294-E20_N5;7770;9170</td>\n",
       "      <td>AM-294-E20_N5;7770;9170</td>\n",
       "      <td>AM-294-E20</td>\n",
       "      <td>d__Bacteria;p__Firmicutes_B;c__Desulfotomaculi...</td>\n",
       "      <td>exclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124814</th>\n",
       "      <td>AM-294-L14_N3;8038;8610</td>\n",
       "      <td>AM-294-E11_N9;13611;14183</td>\n",
       "      <td>AM-294-E11</td>\n",
       "      <td>d__Bacteria;p__Firmicutes_B;c__Desulfotomaculi...</td>\n",
       "      <td>exclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124815</th>\n",
       "      <td>AM-302-A21_N4;6358;7056</td>\n",
       "      <td>AM-302-A21_N4;6358;7056</td>\n",
       "      <td>AM-302-A21</td>\n",
       "      <td>d__Archaea;p__Thermoproteota;c__Bathyarchaeia;...</td>\n",
       "      <td>exclusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124816</th>\n",
       "      <td>AM-302-A21_N4;6358;7056</td>\n",
       "      <td>AM-304-G09_N3;29860;30558</td>\n",
       "      <td>AM-304-G09</td>\n",
       "      <td>d__Archaea;p__Thermoproteota;c__Bathyarchaeia;...</td>\n",
       "      <td>exclusive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124817 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  gene_cluster_rep                        gene    gene_sag  \\\n",
       "0       AM-294-P06_N42;11716;12369  AM-294-P06_N42;11716;12369  AM-294-P06   \n",
       "1       AM-294-P06_N42;11716;12369   AM-307-N22_N5;46105;46758  AM-307-N22   \n",
       "2          AM-294-E20_N5;7770;9170     AM-294-E20_N5;7770;9170  AM-294-E20   \n",
       "...                            ...                         ...         ...   \n",
       "124814     AM-294-L14_N3;8038;8610   AM-294-E11_N9;13611;14183  AM-294-E11   \n",
       "124815     AM-302-A21_N4;6358;7056     AM-302-A21_N4;6358;7056  AM-302-A21   \n",
       "124816     AM-302-A21_N4;6358;7056   AM-304-G09_N3;29860;30558  AM-304-G09   \n",
       "\n",
       "                                               gene_genus  \\\n",
       "0       d__Bacteria;p__Desulfobacterota;c__BSN033;o__B...   \n",
       "1       d__Bacteria;p__Desulfobacterota;c__BSN033;o__B...   \n",
       "2       d__Bacteria;p__Firmicutes_B;c__Desulfotomaculi...   \n",
       "...                                                   ...   \n",
       "124814  d__Bacteria;p__Firmicutes_B;c__Desulfotomaculi...   \n",
       "124815  d__Archaea;p__Thermoproteota;c__Bathyarchaeia;...   \n",
       "124816  d__Archaea;p__Thermoproteota;c__Bathyarchaeia;...   \n",
       "\n",
       "       gene_cluster_genus_status  \n",
       "0                      exclusive  \n",
       "1                      exclusive  \n",
       "2                      exclusive  \n",
       "...                          ...  \n",
       "124814                 exclusive  \n",
       "124815                 exclusive  \n",
       "124816                 exclusive  \n",
       "\n",
       "[124817 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/scgc/simon/microg2p/Melody/BLM1/Analyses/RNA_GORG_recruit/annotations/cDNA4-A01_joined_annotated.txt.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/scgc/scgc_nfs/opt/common/anaconda3a/envs/jacob_env/lib/python3.6/site-packages/ipykernel_launcher.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/mnt/scgc/scgc_nfs/opt/common/anaconda3a/envs/jacob_env/lib/python3.6/site-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/mnt/scgc/scgc_nfs/opt/common/anaconda3a/envs/jacob_env/lib/python3.6/site-packages/ipykernel_launcher.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/mnt/scgc/scgc_nfs/opt/common/anaconda3a/envs/jacob_env/lib/python3.6/site-packages/ipykernel_launcher.py:71: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of processed reads is 1000000 the total number of classified reads is 4660.0 0.466 % of reads recruiting\n",
      "the number of processed reads is 2000000 the total number of classified reads is 9713.0 0.48564999999999997 % of reads recruiting\n",
      "the number of processed reads is 3000000 the total number of classified reads is 14990.0 0.49966666666666665 % of reads recruiting\n",
      "the number of processed reads is 4000000 the total number of classified reads is 20171.0 0.504275 % of reads recruiting\n",
      "the number of processed reads is 5000000 the total number of classified reads is 25384.0 0.50768 % of reads recruiting\n",
      "the number of processed reads is 6000000 the total number of classified reads is 30690.0 0.5115 % of reads recruiting\n",
      "the number of processed reads is 7000000 the total number of classified reads is 35870.0 0.5124285714285715 % of reads recruiting\n",
      "the number of processed reads is 8000000 the total number of classified reads is 41069.0 0.5133625 % of reads recruiting\n",
      "the number of processed reads is 9000000 the total number of classified reads is 46291.0 0.5143444444444445 % of reads recruiting\n",
      "the number of processed reads is 10000000 the total number of classified reads is 51370.0 0.5137 % of reads recruiting\n",
      "the number of processed reads is 11000000 the total number of classified reads is 56524.0 0.5138545454545455 % of reads recruiting\n",
      "the number of processed reads is 12000000 the total number of classified reads is 61813.0 0.5151083333333333 % of reads recruiting\n",
      "the number of processed reads is 13000000 the total number of classified reads is 66945.0 0.5149615384615385 % of reads recruiting\n",
      "the number of processed reads is 14000000 the total number of classified reads is 72111.0 0.5150785714285715 % of reads recruiting\n",
      "the number of processed reads is 15000000 the total number of classified reads is 77315.0 0.5154333333333334 % of reads recruiting\n",
      "the number of processed reads is 16000000 the total number of classified reads is 82537.0 0.51585625 % of reads recruiting\n",
      "/mnt/scgc/simon/microg2p/Melody/BLM1/Analyses/RNA_GORG_recruit/annotations/cDNA6-A01_joined_annotated.txt.gz\n",
      "/mnt/scgc/simon/microg2p/Melody/BLM1/Analyses/RNA_GORG_recruit/annotations/cDNA9-A01_joined_annotated.txt.gz\n",
      "/mnt/scgc/simon/microg2p/Melody/BLM1/Analyses/RNA_GORG_recruit/annotations/cDNA2-A01_joined_annotated.txt.gz\n",
      "the number of processed reads is 1000000 the total number of classified reads is 536.0 0.0536 % of reads recruiting\n",
      "the number of processed reads is 2000000 the total number of classified reads is 1121.0 0.05605 % of reads recruiting\n",
      "the number of processed reads is 3000000 the total number of classified reads is 1769.0 0.05896666666666666 % of reads recruiting\n",
      "the number of processed reads is 4000000 the total number of classified reads is 2408.0 0.0602 % of reads recruiting\n",
      "the number of processed reads is 5000000 the total number of classified reads is 2995.0 0.0599 % of reads recruiting\n",
      "the number of processed reads is 6000000 the total number of classified reads is 3627.0 0.060450000000000004 % of reads recruiting\n",
      "the number of processed reads is 7000000 the total number of classified reads is 4199.0 0.05998571428571429 % of reads recruiting\n",
      "the number of processed reads is 8000000 the total number of classified reads is 4796.0 0.059949999999999996 % of reads recruiting\n",
      "the number of processed reads is 9000000 the total number of classified reads is 5442.0 0.06046666666666667 % of reads recruiting\n",
      "the number of processed reads is 10000000 the total number of classified reads is 6064.0 0.06064 % of reads recruiting\n",
      "the number of processed reads is 11000000 the total number of classified reads is 6672.0 0.060654545454545454 % of reads recruiting\n",
      "the number of processed reads is 12000000 the total number of classified reads is 7314.0 0.060950000000000004 % of reads recruiting\n",
      "the number of processed reads is 13000000 the total number of classified reads is 7888.0 0.06067692307692308 % of reads recruiting\n",
      "the number of processed reads is 14000000 the total number of classified reads is 8524.0 0.060885714285714286 % of reads recruiting\n",
      "the number of processed reads is 15000000 the total number of classified reads is 9130.0 0.060866666666666666 % of reads recruiting\n",
      "/mnt/scgc/simon/microg2p/Melody/BLM1/Analyses/RNA_GORG_recruit/annotations/cDNA3-A01_joined_annotated.txt.gz\n",
      "the number of processed reads is 1000000 the total number of classified reads is 24 0.0024000000000000002 % of reads recruiting\n",
      "the number of processed reads is 2000000 the total number of classified reads is 51 0.00255 % of reads recruiting\n",
      "the number of processed reads is 3000000 the total number of classified reads is 86.0 0.0028666666666666667 % of reads recruiting\n",
      "the number of processed reads is 4000000 the total number of classified reads is 110.0 0.0027500000000000003 % of reads recruiting\n",
      "/mnt/scgc/simon/microg2p/Melody/BLM1/Analyses/RNA_GORG_recruit/annotations/cDNA7-A01_joined_annotated.txt.gz\n",
      "the number of processed reads is 1000000 the total number of classified reads is 28692.0 2.8691999999999998 % of reads recruiting\n",
      "the number of processed reads is 2000000 the total number of classified reads is 60536.0 3.0268 % of reads recruiting\n",
      "the number of processed reads is 3000000 the total number of classified reads is 92187.0 3.0728999999999997 % of reads recruiting\n",
      "the number of processed reads is 4000000 the total number of classified reads is 123911.0 3.097775 % of reads recruiting\n",
      "the number of processed reads is 5000000 the total number of classified reads is 155487.0 3.10974 % of reads recruiting\n",
      "the number of processed reads is 6000000 the total number of classified reads is 187021.0 3.1170166666666663 % of reads recruiting\n",
      "the number of processed reads is 7000000 the total number of classified reads is 218453.0 3.120757142857143 % of reads recruiting\n",
      "the number of processed reads is 8000000 the total number of classified reads is 249931.0 3.1241375 % of reads recruiting\n",
      "the number of processed reads is 9000000 the total number of classified reads is 281130.0 3.123666666666667 % of reads recruiting\n",
      "the number of processed reads is 10000000 the total number of classified reads is 312846.0 3.1284600000000005 % of reads recruiting\n",
      "the number of processed reads is 11000000 the total number of classified reads is 344553.0 3.1323 % of reads recruiting\n",
      "the number of processed reads is 12000000 the total number of classified reads is 376304.0 3.135866666666667 % of reads recruiting\n",
      "the number of processed reads is 13000000 the total number of classified reads is 408157.0 3.1396692307692304 % of reads recruiting\n",
      "the number of processed reads is 14000000 the total number of classified reads is 439694.0 3.140671428571429 % of reads recruiting\n",
      "the number of processed reads is 15000000 the total number of classified reads is 471205.0 3.1413666666666664 % of reads recruiting\n",
      "the number of processed reads is 16000000 the total number of classified reads is 503553.0 3.14720625 % of reads recruiting\n",
      "the number of processed reads is 17000000 the total number of classified reads is 535772.0 3.1516 % of reads recruiting\n",
      "the number of processed reads is 18000000 the total number of classified reads is 567813.0 3.1545166666666664 % of reads recruiting\n",
      "the number of processed reads is 19000000 the total number of classified reads is 599877.0 3.1572473684210527 % of reads recruiting\n",
      "the number of processed reads is 20000000 the total number of classified reads is 631219.0 3.1560949999999997 % of reads recruiting\n",
      "the number of processed reads is 21000000 the total number of classified reads is 662576.0 3.15512380952381 % of reads recruiting\n",
      "the number of processed reads is 22000000 the total number of classified reads is 693989.0 3.1544954545454544 % of reads recruiting\n",
      "the number of processed reads is 23000000 the total number of classified reads is 725628.0 3.1549043478260868 % of reads recruiting\n",
      "the number of processed reads is 24000000 the total number of classified reads is 756974.0 3.154058333333333 % of reads recruiting\n",
      "the number of processed reads is 25000000 the total number of classified reads is 788631.0 3.1545240000000003 % of reads recruiting\n",
      "the number of processed reads is 26000000 the total number of classified reads is 819940.0 3.1536153846153847 % of reads recruiting\n",
      "the number of processed reads is 27000000 the total number of classified reads is 851549.0 3.1538851851851852 % of reads recruiting\n",
      "the number of processed reads is 28000000 the total number of classified reads is 883176.0 3.1542 % of reads recruiting\n",
      "the number of processed reads is 29000000 the total number of classified reads is 914808.0 3.1545103448275857 % of reads recruiting\n",
      "the number of processed reads is 30000000 the total number of classified reads is 946178.0 3.153926666666667 % of reads recruiting\n",
      "the number of processed reads is 31000000 the total number of classified reads is 977798.0 3.1541870967741934 % of reads recruiting\n",
      "the number of processed reads is 32000000 the total number of classified reads is 1009226.0 3.1538312499999996 % of reads recruiting\n",
      "the number of processed reads is 33000000 the total number of classified reads is 1040971.0 3.1544575757575757 % of reads recruiting\n",
      "the number of processed reads is 34000000 the total number of classified reads is 1072836.0 3.1553999999999998 % of reads recruiting\n",
      "the number of processed reads is 35000000 the total number of classified reads is 1104452.0 3.155577142857143 % of reads recruiting\n",
      "the number of processed reads is 36000000 the total number of classified reads is 1136543.0 3.157063888888889 % of reads recruiting\n",
      "the number of processed reads is 37000000 the total number of classified reads is 1168500.0 3.158108108108108 % of reads recruiting\n",
      "the number of processed reads is 38000000 the total number of classified reads is 1200147.0 3.158281578947368 % of reads recruiting\n",
      "the number of processed reads is 39000000 the total number of classified reads is 1232083.0 3.1591871794871795 % of reads recruiting\n",
      "/mnt/scgc/simon/microg2p/Melody/BLM1/Analyses/RNA_GORG_recruit/annotations/cDNA1-A01_joined_annotated.txt.gz\n",
      "the number of processed reads is 1000000 the total number of classified reads is 3452.0 0.3452 % of reads recruiting\n",
      "the number of processed reads is 2000000 the total number of classified reads is 7307.0 0.36535 % of reads recruiting\n",
      "the number of processed reads is 3000000 the total number of classified reads is 11200.0 0.37333333333333335 % of reads recruiting\n",
      "the number of processed reads is 4000000 the total number of classified reads is 15087.0 0.377175 % of reads recruiting\n",
      "the number of processed reads is 5000000 the total number of classified reads is 18988.0 0.37976 % of reads recruiting\n",
      "the number of processed reads is 6000000 the total number of classified reads is 22883.0 0.3813833333333333 % of reads recruiting\n",
      "the number of processed reads is 7000000 the total number of classified reads is 26707.0 0.38152857142857144 % of reads recruiting\n",
      "the number of processed reads is 8000000 the total number of classified reads is 30641.0 0.38301250000000003 % of reads recruiting\n",
      "the number of processed reads is 9000000 the total number of classified reads is 34622.0 0.38468888888888886 % of reads recruiting\n",
      "the number of processed reads is 10000000 the total number of classified reads is 38580.0 0.3858 % of reads recruiting\n",
      "the number of processed reads is 11000000 the total number of classified reads is 42537.0 0.38670000000000004 % of reads recruiting\n",
      "the number of processed reads is 12000000 the total number of classified reads is 46490.0 0.3874166666666667 % of reads recruiting\n",
      "the number of processed reads is 13000000 the total number of classified reads is 50420.0 0.38784615384615384 % of reads recruiting\n",
      "the number of processed reads is 14000000 the total number of classified reads is 54288.0 0.38777142857142854 % of reads recruiting\n",
      "the number of processed reads is 15000000 the total number of classified reads is 58216.0 0.38810666666666666 % of reads recruiting\n",
      "the number of processed reads is 16000000 the total number of classified reads is 61982.0 0.3873875 % of reads recruiting\n",
      "the number of processed reads is 17000000 the total number of classified reads is 65891.0 0.3875941176470588 % of reads recruiting\n",
      "the number of processed reads is 18000000 the total number of classified reads is 69769.0 0.3876055555555556 % of reads recruiting\n",
      "the number of processed reads is 19000000 the total number of classified reads is 73647.0 0.38761578947368425 % of reads recruiting\n",
      "the number of processed reads is 20000000 the total number of classified reads is 77530.0 0.38765 % of reads recruiting\n",
      "the number of processed reads is 21000000 the total number of classified reads is 81392.0 0.3875809523809524 % of reads recruiting\n",
      "the number of processed reads is 22000000 the total number of classified reads is 85279.0 0.3876318181818182 % of reads recruiting\n",
      "the number of processed reads is 23000000 the total number of classified reads is 89209.0 0.38786521739130436 % of reads recruiting\n",
      "DONE\n",
      "It took --- 206.70216941833496 seconds --- to process all of the RNA reads\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# specify columns that need to have whitespaces stripped\n",
    "cols = ['taxonomic_lineage', 'gene_cluster_rep']\n",
    "\n",
    "for file in read_files:\n",
    "    print(file)\n",
    "    \n",
    "    # extract the filename and modify it to use as the output file\n",
    "    base=os.path.basename(file)\n",
    "    base=base[:-7]\n",
    "    out_name=(base+'_RNA_GORG_results.csv')\n",
    "    \n",
    "    \n",
    "    chunksize = 100000\n",
    "    counter=1\n",
    "    check=1\n",
    "    # read the file in chunks and group the reads by each gene/tax lineage for each chunk\n",
    "    for chunk in pd.read_csv(file, compression='gzip', sep='\\t', chunksize=chunksize, skiprows=0, names=header):\n",
    "        # tracker to keep track of the progress\n",
    "        if counter % 10 == 0:\n",
    "            print('the number of processed reads is',counter*chunksize, 'the total number of classified reads is', DF_summary.number_of_reads.sum(), DF_summary.number_of_reads.sum()/(counter*chunksize)*100, '% of reads recruiting')\n",
    "            \n",
    "        # read in the first chunk process it and store it as a summary df\n",
    "        if check==1:\n",
    "            # extract classified reads only\n",
    "            DF_summary = chunk[(chunk['status'] == 'C')]\n",
    "            if len(DF_summary)==0:\n",
    "                # for some of the samples there are no classified reads this check makes sure that this first part is repeated\n",
    "                # until there is at least one classified read and then it moves on to the second part that stacks the chunks\n",
    "                check=1\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                check=2\n",
    "                \n",
    "                # extract the first gene that the read is assigned to\n",
    "                DF_summary['sequence_ids_lca']=[i.split(\",\")[0] if \",\" in i else i for i in DF_summary['sequence_ids_lca']]\n",
    "\n",
    "                # for these samples only I have to modify the gene name to add the '-' back in\n",
    "                DF_summary['sequence_ids_lca']=DF_summary['sequence_ids_lca'].str[:2] + '-' + DF_summary['sequence_ids_lca'].str[2:5] + '-'+DF_summary['sequence_ids_lca'].str[5:]\n",
    "            \n",
    "                # merge the file with the gcdf which has the gene_cluster_rep (a unique identifier) for every gene\n",
    "                DF_summary=DF_summary.merge(gcdf2, left_on='sequence_ids_lca', right_on='gene', how='left')\n",
    "\n",
    "                # extract the gene annotation data from the summary file and drop duplicates\n",
    "                genes_summary=DF_summary[['gene_cluster_rep', 'prokka_EC_number', 'prokka_product', 'swissprot_gene',\n",
    "                                          'swissprot_EC_number', 'swissprot_eggNOG', 'swissprot_KO', 'swissprot_Pfam',\n",
    "                                          'swissprot_CAZy', 'swissprot_TIGRFAMs']].copy()\n",
    "                genes_summary.drop_duplicates(inplace=True)\n",
    "                \n",
    "                \n",
    "                # group the df by taxonomic lineage, and gene cluster\n",
    "                DF_summary=DF_summary.groupby(['taxonomic_lineage', 'gene_cluster_rep'], as_index=False, dropna=False)['status'].count()\n",
    "                DF_summary.rename(columns={'status':'number_of_reads'}, inplace=True)\n",
    "                                \n",
    "                \n",
    "        # read all other chunks into a temporary df and process them\n",
    "        else:\n",
    "            # Same as above but store each successive chunk as a tempory_df\n",
    "            tdf_summary = chunk[(chunk['status'] == 'C')]\n",
    "            if len(tdf_summary)==0:\n",
    "                # proceed to the next chunk if there are no classified reads\n",
    "                continue\n",
    "            \n",
    "            else:               \n",
    "                \n",
    "                tdf_summary['sequence_ids_lca']=[i.split(\",\")[0] if \",\" in i else i for i in tdf_summary['sequence_ids_lca']]\n",
    "                # for these samples only I have to modify the gene name to add the '-' back in\n",
    "                tdf_summary['sequence_ids_lca']=tdf_summary['sequence_ids_lca'].str[:2] + '-' + tdf_summary['sequence_ids_lca'].str[2:5] + '-'+tdf_summary['sequence_ids_lca'].str[5:]\n",
    "            \n",
    "                tdf_summary=tdf_summary.merge(gcdf2, left_on='sequence_ids_lca', right_on='gene', how='left')\n",
    "        \n",
    "                # extract all the gene information from this chuink and drop duplicates\n",
    "                tgenes_summary=tdf_summary[['gene_cluster_rep', 'prokka_EC_number', 'prokka_product', 'swissprot_gene',\n",
    "                                          'swissprot_EC_number', 'swissprot_eggNOG', 'swissprot_KO', 'swissprot_Pfam', \n",
    "                                          'swissprot_CAZy', 'swissprot_TIGRFAMs']].copy()\n",
    "                tgenes_summary.drop_duplicates(inplace=True)\n",
    "                \n",
    "                \n",
    "                tdf_summary=tdf_summary.groupby(['taxonomic_lineage', 'gene_cluster_rep'], as_index=False, dropna=False)['status'].count()\n",
    "                tdf_summary.rename(columns={'status':'number_of_reads'}, inplace=True)\n",
    "               \n",
    "                \n",
    "                # merge the temporary df with the summary df and convert nan to 0 (this allows for adding togehter thecolumns)\n",
    "                DF_summary=DF_summary.merge(tdf_summary, on=['taxonomic_lineage', 'gene_cluster_rep'], how='outer')\n",
    "                DF_summary[\"number_of_reads_x\"].fillna(0, inplace=True)\n",
    "                DF_summary[\"number_of_reads_y\"].fillna(0, inplace=True)\n",
    "        \n",
    "                # add the read numbers, and drop the read number columns that were duplicated upon the merge\n",
    "                DF_summary['number_of_reads']=DF_summary['number_of_reads_x']+DF_summary['number_of_reads_y']\n",
    "                DF_summary.drop(columns=['number_of_reads_x', 'number_of_reads_y'], inplace=True)\n",
    "                \n",
    "                \n",
    "                \n",
    "                # stack gene data with initial gene data and drop duplicates This will be appended at the end\n",
    "                genes_summary=pd.concat([genes_summary, tgenes_summary], ignore_index=True, axis=0)\n",
    "                genes_summary.drop_duplicates(inplace=True)\n",
    "                \n",
    "\n",
    "        counter+=1\n",
    "        \n",
    "    # add the gene data\n",
    "    if check==2:\n",
    "        \n",
    "        genes=genes_summary['gene_cluster_rep'].unique()\n",
    "        final_genes_summary=pd.DataFrame()\n",
    "        for i in genes:\n",
    "            tdf_genes=genes_summary[genes_summary['gene_cluster_rep']==i]\n",
    "            # if there is only one product use that\n",
    "            if len(tdf_genes['prokka_product'].unique())==1:\n",
    "                final_genes_summary=pd.concat([final_genes_summary, tdf_genes])\n",
    "            # if there are more than one product get rid of the hypothetical\n",
    "            else:\n",
    "                tdf_genes=tdf_genes[tdf_genes['prokka_product']!='hypothetical protein']\n",
    "                if len(tdf_genes['prokka_product'].unique())>1:\n",
    "                    # if there are still multiple products use only the first one\n",
    "                    tdf_genes=tdf_genes.iloc[0]\n",
    "                    final_genes_summary=pd.concat([final_genes_summary, tdf_genes])\n",
    "                    \n",
    "                else:\n",
    "                    final_genes_summary=pd.concat([final_genes_summary, tdf_genes])\n",
    "                \n",
    "        \n",
    "        DF_summary=DF_summary.merge(final_genes_summary, on='gene_cluster_rep', how='left')\n",
    "        DF_summary.to_csv(os.path.join(outdir, out_name))\n",
    "    \n",
    "print('DONE')\n",
    "\n",
    "print(\"It took --- %s seconds --- to process all of the RNA reads\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The noncompetitive version below was very specific for Melodies project where she wanted to identify all cells that a read recruited to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/scgc/simon/microg2p/Melody/BLM1/Analyses/RNA_GORG_recruit/annotations/cDNA4-A01_joined_annotated.txt.gz\n",
      "the number of processed reads is 1000000\n",
      "the number of processed reads is 2000000\n",
      "the number of processed reads is 3000000\n",
      "the number of processed reads is 4000000\n",
      "the number of processed reads is 5000000\n",
      "the number of processed reads is 6000000\n",
      "the number of processed reads is 7000000\n",
      "the number of processed reads is 8000000\n",
      "the number of processed reads is 9000000\n",
      "the number of processed reads is 10000000\n",
      "the number of processed reads is 11000000\n",
      "the number of processed reads is 12000000\n",
      "the number of processed reads is 13000000\n",
      "the number of processed reads is 14000000\n",
      "the number of processed reads is 15000000\n",
      "the number of processed reads is 16000000\n",
      "/mnt/scgc/simon/microg2p/Melody/BLM1/Analyses/RNA_GORG_recruit/annotations/cDNA6-A01_joined_annotated.txt.gz\n",
      "/mnt/scgc/simon/microg2p/Melody/BLM1/Analyses/RNA_GORG_recruit/annotations/cDNA9-A01_joined_annotated.txt.gz\n",
      "/mnt/scgc/simon/microg2p/Melody/BLM1/Analyses/RNA_GORG_recruit/annotations/cDNA2-A01_joined_annotated.txt.gz\n",
      "the number of processed reads is 1000000\n",
      "the number of processed reads is 2000000\n",
      "the number of processed reads is 3000000\n",
      "the number of processed reads is 4000000\n",
      "the number of processed reads is 5000000\n",
      "the number of processed reads is 6000000\n",
      "the number of processed reads is 7000000\n",
      "the number of processed reads is 8000000\n",
      "the number of processed reads is 9000000\n",
      "the number of processed reads is 10000000\n",
      "the number of processed reads is 11000000\n",
      "the number of processed reads is 12000000\n",
      "the number of processed reads is 13000000\n",
      "the number of processed reads is 14000000\n",
      "the number of processed reads is 15000000\n",
      "/mnt/scgc/simon/microg2p/Melody/BLM1/Analyses/RNA_GORG_recruit/annotations/cDNA3-A01_joined_annotated.txt.gz\n",
      "the number of processed reads is 1000000\n",
      "the number of processed reads is 2000000\n",
      "the number of processed reads is 3000000\n",
      "the number of processed reads is 4000000\n",
      "/mnt/scgc/simon/microg2p/Melody/BLM1/Analyses/RNA_GORG_recruit/annotations/cDNA7-A01_joined_annotated.txt.gz\n",
      "the number of processed reads is 1000000\n",
      "the number of processed reads is 2000000\n",
      "the number of processed reads is 3000000\n",
      "the number of processed reads is 4000000\n",
      "the number of processed reads is 5000000\n",
      "the number of processed reads is 6000000\n",
      "the number of processed reads is 7000000\n",
      "the number of processed reads is 8000000\n",
      "the number of processed reads is 9000000\n",
      "the number of processed reads is 10000000\n",
      "the number of processed reads is 11000000\n",
      "the number of processed reads is 12000000\n",
      "the number of processed reads is 13000000\n",
      "the number of processed reads is 14000000\n",
      "the number of processed reads is 15000000\n",
      "the number of processed reads is 16000000\n",
      "the number of processed reads is 17000000\n",
      "the number of processed reads is 18000000\n",
      "the number of processed reads is 19000000\n",
      "the number of processed reads is 20000000\n",
      "the number of processed reads is 21000000\n",
      "the number of processed reads is 22000000\n",
      "the number of processed reads is 23000000\n",
      "the number of processed reads is 24000000\n",
      "the number of processed reads is 25000000\n",
      "the number of processed reads is 26000000\n",
      "the number of processed reads is 27000000\n",
      "the number of processed reads is 28000000\n",
      "the number of processed reads is 29000000\n",
      "the number of processed reads is 30000000\n",
      "the number of processed reads is 31000000\n",
      "the number of processed reads is 32000000\n",
      "the number of processed reads is 33000000\n",
      "the number of processed reads is 34000000\n",
      "the number of processed reads is 35000000\n",
      "the number of processed reads is 36000000\n",
      "the number of processed reads is 37000000\n",
      "the number of processed reads is 38000000\n",
      "the number of processed reads is 39000000\n",
      "/mnt/scgc/simon/microg2p/Melody/BLM1/Analyses/RNA_GORG_recruit/annotations/cDNA1-A01_joined_annotated.txt.gz\n",
      "the number of processed reads is 1000000\n",
      "the number of processed reads is 2000000\n",
      "the number of processed reads is 3000000\n",
      "the number of processed reads is 4000000\n",
      "the number of processed reads is 5000000\n",
      "the number of processed reads is 6000000\n",
      "the number of processed reads is 7000000\n",
      "the number of processed reads is 8000000\n",
      "the number of processed reads is 9000000\n",
      "the number of processed reads is 10000000\n",
      "the number of processed reads is 11000000\n",
      "the number of processed reads is 12000000\n",
      "the number of processed reads is 13000000\n",
      "the number of processed reads is 14000000\n",
      "the number of processed reads is 15000000\n",
      "the number of processed reads is 16000000\n",
      "the number of processed reads is 17000000\n",
      "the number of processed reads is 18000000\n",
      "the number of processed reads is 19000000\n",
      "the number of processed reads is 20000000\n",
      "the number of processed reads is 21000000\n",
      "the number of processed reads is 22000000\n",
      "the number of processed reads is 23000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAG</th>\n",
       "      <th>sequence_ids_lca</th>\n",
       "      <th>number_of_reads</th>\n",
       "      <th>prokka_EC_number</th>\n",
       "      <th>prokka_product</th>\n",
       "      <th>swissprot_gene</th>\n",
       "      <th>swissprot_EC_number</th>\n",
       "      <th>swissprot_eggNOG</th>\n",
       "      <th>swissprot_KO</th>\n",
       "      <th>swissprot_Pfam</th>\n",
       "      <th>swissprot_CAZy</th>\n",
       "      <th>swissprot_TIGRFAMs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AM-294-B10</td>\n",
       "      <td>AM294B10_N17;3470;4603</td>\n",
       "      <td>18607.0</td>\n",
       "      <td>6.3.5.5</td>\n",
       "      <td>Carbamoyl-phosphate synthase small chain</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ENOG4105C1M,CCOG0505</td>\n",
       "      <td>K01956</td>\n",
       "      <td>PF00988,CPF00117</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TIGR01368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AM-294-D05</td>\n",
       "      <td>AM294D05_N11;8880;10043</td>\n",
       "      <td>18611.0</td>\n",
       "      <td>6.3.5.5</td>\n",
       "      <td>Carbamoyl-phosphate synthase small chain</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ENOG4105C1M,CCOG0505</td>\n",
       "      <td>K01956</td>\n",
       "      <td>PF00988,CPF00117</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TIGR01368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AM-294-L10</td>\n",
       "      <td>AM294L10_N20;891;1535</td>\n",
       "      <td>7457.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hypothetical protein</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>AM-294-G17</td>\n",
       "      <td>AM294G17_N4;50156;50374</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Putative nitrogen fixation protein YutI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ENOG41083MR,CCOG0694</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PF01106</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>AM-294-J06</td>\n",
       "      <td>AM294J06_N23;5746;6825</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.3.5.5</td>\n",
       "      <td>Carbamoyl-phosphate synthase pyrimidine-specif...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PF00988,CPF00117</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TIGR01368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>AM-296-B22</td>\n",
       "      <td>AM296B22_N14;2166;2447</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hypothetical protein</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           SAG         sequence_ids_lca  number_of_reads prokka_EC_number  \\\n",
       "0   AM-294-B10   AM294B10_N17;3470;4603          18607.0          6.3.5.5   \n",
       "1   AM-294-D05  AM294D05_N11;8880;10043          18611.0          6.3.5.5   \n",
       "2   AM-294-L10    AM294L10_N20;891;1535           7457.0              NaN   \n",
       "..         ...                      ...              ...              ...   \n",
       "19  AM-294-G17  AM294G17_N4;50156;50374              1.0              NaN   \n",
       "20  AM-294-J06   AM294J06_N23;5746;6825              1.0          6.3.5.5   \n",
       "21  AM-296-B22   AM296B22_N14;2166;2447              1.0              NaN   \n",
       "\n",
       "                                       prokka_product swissprot_gene  \\\n",
       "0            Carbamoyl-phosphate synthase small chain            NaN   \n",
       "1            Carbamoyl-phosphate synthase small chain            NaN   \n",
       "2                                hypothetical protein            NaN   \n",
       "..                                                ...            ...   \n",
       "19            Putative nitrogen fixation protein YutI            NaN   \n",
       "20  Carbamoyl-phosphate synthase pyrimidine-specif...            NaN   \n",
       "21                               hypothetical protein            NaN   \n",
       "\n",
       "   swissprot_EC_number      swissprot_eggNOG swissprot_KO    swissprot_Pfam  \\\n",
       "0                  NaN  ENOG4105C1M,CCOG0505       K01956  PF00988,CPF00117   \n",
       "1                  NaN  ENOG4105C1M,CCOG0505       K01956  PF00988,CPF00117   \n",
       "2                  NaN                   NaN          NaN               NaN   \n",
       "..                 ...                   ...          ...               ...   \n",
       "19                 NaN  ENOG41083MR,CCOG0694          NaN           PF01106   \n",
       "20                 NaN                   NaN          NaN  PF00988,CPF00117   \n",
       "21                 NaN                   NaN          NaN               NaN   \n",
       "\n",
       "   swissprot_CAZy swissprot_TIGRFAMs  \n",
       "0             NaN          TIGR01368  \n",
       "1             NaN          TIGR01368  \n",
       "2             NaN                NaN  \n",
       "..            ...                ...  \n",
       "19            NaN                NaN  \n",
       "20            NaN          TIGR01368  \n",
       "21            NaN                NaN  \n",
       "\n",
       "[22 rows x 12 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Redo this in a noncompetitive version so that there is a document that has all the reads to genes in SAGs.\n",
    "\n",
    "start_time = time.time()\n",
    "for file in read_files:\n",
    "    print(file)\n",
    "    \n",
    "    # extract the filename and modify it to use as the output file\n",
    "    base=os.path.basename(file)\n",
    "    base=base[:-7]\n",
    "    out_name=(base+'_noncompetitive_RNA_GORG_results.csv')\n",
    "    \n",
    "    \n",
    "    chunksize = 100000\n",
    "    counter=1\n",
    "    check=1\n",
    "    # read the file in chunks and group the reads by each gene/tax lineage for each chunk\n",
    "    for chunk in pd.read_csv(file, compression='gzip', sep='\\t', chunksize=chunksize, skiprows=0, names=header):\n",
    "        # tracker to keep track of the progress\n",
    "        if counter % 10 == 0:\n",
    "            print('the number of processed reads is',counter*chunksize)\n",
    "            \n",
    "        # read in the first chunk process it and store it as a summary df\n",
    "        if check==1:\n",
    "            # extract classified reads only\n",
    "            DF_summary = chunk[(chunk['status'] == 'C')]\n",
    "            if len(DF_summary)==0:\n",
    "                check=1\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                check=2\n",
    "\n",
    "\n",
    "  # copy the df into a new df that I can use to generate noncompetitive counts of all the SAGs that a read was assigned to\n",
    "                DF_explode=DF_summary.copy()\n",
    "\n",
    "                DF_explode=DF_explode.set_index(['status', 'sequence_id', 'length', 'taxonomy_id', 'taxonomy_ids_lca', 'protein_sequence', 'taxonomic_lineage', 'prokka_EC_number', 'prokka_product', 'swissprot_gene', 'swissprot_EC_number', 'swissprot_eggNOG', 'swissprot_KO', 'swissprot_Pfam', 'swissprot_CAZy', 'swissprot_TIGRFAMs']).apply(lambda x: x.str.split(',').explode()).reset_index()\n",
    "                DF_explode['SAG']=DF_explode['sequence_ids_lca'].str[:2] + '-' + DF_explode['sequence_ids_lca'].str[2:5] + '-'+DF_explode['sequence_ids_lca'].str[5:8]\n",
    "                # for some reason this adds an extra row for each read the is recruited so I am getting rid of that.\n",
    "                DF_explode=DF_explode[DF_explode['SAG']!='--']\n",
    "                \n",
    "                # extract gene annotations for per SAG read counts\n",
    "                DF_explode_gene_annotations=DF_explode[['prokka_EC_number', 'prokka_product', 'swissprot_gene', 'swissprot_EC_number', 'swissprot_eggNOG', 'swissprot_KO', 'swissprot_Pfam', 'swissprot_CAZy', 'swissprot_TIGRFAMs', 'sequence_ids_lca']].copy()\n",
    "                \n",
    "                DF_explode_summary=DF_explode.groupby(['SAG', 'sequence_ids_lca'], as_index=False, dropna=False)['status'].count()\n",
    "                DF_explode_summary.rename(columns={'status':'number_of_reads'}, inplace=True)\n",
    "\n",
    "\n",
    "                \n",
    " # read all other chunks into a temporary df and \n",
    "        else:\n",
    "            # Same as above but store each successive chunk as a tempory_df\n",
    "            tdf_summary = chunk[(chunk['status'] == 'C')]\n",
    "            if len(tdf_summary)==0:\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            else:\n",
    "                # copy the df into a new df that I can use to generate noncompetitive counts of all the SAGs that a read was assigned to\n",
    "                tDF_explode=tdf_summary.copy()\n",
    "\n",
    "                tDF_explode=tDF_explode.set_index(['status', 'sequence_id', 'length', 'taxonomy_id', 'taxonomy_ids_lca', 'protein_sequence', 'taxonomic_lineage', 'prokka_EC_number', 'prokka_product', 'swissprot_gene', 'swissprot_EC_number', 'swissprot_eggNOG', 'swissprot_KO', 'swissprot_Pfam', 'swissprot_CAZy', 'swissprot_TIGRFAMs']).apply(lambda x: x.str.split(',').explode()).reset_index()\n",
    "                tDF_explode['SAG']=tDF_explode['sequence_ids_lca'].str[:2] + '-' + tDF_explode['sequence_ids_lca'].str[2:5] + '-'+tDF_explode['sequence_ids_lca'].str[5:8]\n",
    "                # for some reason this adds an extra row so I am getting rid of that.\n",
    "                tDF_explode=tDF_explode[tDF_explode['SAG']!='--']\n",
    "                \n",
    "                tDF_explode_gene_annotations=tDF_explode[['prokka_EC_number', 'prokka_product', 'swissprot_gene', 'swissprot_EC_number', 'swissprot_eggNOG', 'swissprot_KO', 'swissprot_Pfam', 'swissprot_CAZy', 'swissprot_TIGRFAMs', 'sequence_ids_lca']].copy()\n",
    "                \n",
    "                DF_explode_gene_annotations=pd.concat([DF_explode_gene_annotations, tDF_explode_gene_annotations], ignore_index=True, axis=0)\n",
    "                DF_explode_gene_annotations.drop_duplicates(inplace=True)\n",
    "                \n",
    "                tDF_explode_summary=tDF_explode.groupby(['SAG', 'sequence_ids_lca'], as_index=False, dropna=False)['status'].count()\n",
    "                tDF_explode_summary.rename(columns={'status':'number_of_reads'}, inplace=True)\n",
    "\n",
    "                \n",
    "                ### I need to merge the tmp explode df and sum the values and I think I might be done\n",
    "                DF_explode_summary=DF_explode_summary.merge(tDF_explode_summary, on=['SAG', 'sequence_ids_lca'], how='outer')\n",
    "                DF_explode_summary[\"number_of_reads_x\"].fillna(0, inplace=True)\n",
    "                DF_explode_summary[\"number_of_reads_y\"].fillna(0, inplace=True)\n",
    "        \n",
    "                # add the read numbers, and drop the read number columns that were duplicated upon the merge\n",
    "                DF_explode_summary['number_of_reads']=DF_explode_summary['number_of_reads_x']+DF_explode_summary['number_of_reads_y']\n",
    "                DF_explode_summary.drop(columns=['number_of_reads_x', 'number_of_reads_y'], inplace=True)\n",
    "        \n",
    "        counter+=1\n",
    "        \n",
    "\n",
    "    if check==2:\n",
    "        \n",
    "        genes=DF_explode_gene_annotations['sequence_ids_lca'].unique()\n",
    "        final_genes_summary=pd.DataFrame()\n",
    "        for i in genes:\n",
    "            tdf_genes=DF_explode_gene_annotations[DF_explode_gene_annotations['sequence_ids_lca']==i]\n",
    "            if len(tdf_genes['prokka_product'].unique())==1:\n",
    "                final_genes_summary=pd.concat([final_genes_summary, tdf_genes])\n",
    "            # if there are more than one product get rid of the hypothetical\n",
    "            else:\n",
    "                tdf_genes=tdf_genes[tdf_genes['prokka_product']!='hypothetical protein']\n",
    "                if len(tdf_genes['prokka_product'].unique())>1:\n",
    "                    # if there are still multiple products use only the first one\n",
    "                    tdf_genes=tdf_genes.iloc[0]\n",
    "                    final_genes_summary=pd.concat([final_genes_summary, tdf_genes])\n",
    "                    \n",
    "                else:\n",
    "                    final_genes_summary=pd.concat([final_genes_summary, tdf_genes])\n",
    "                \n",
    "                \n",
    "        DF_explode_summary=DF_explode_summary.merge(final_genes_summary, on='sequence_ids_lca', how='left')\n",
    "        DF_explode_summary.to_csv(os.path.join(outdir, out_name))\n",
    "\n",
    "\n",
    "DF_explode_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jacob_env (julia-clone)",
   "language": "python",
   "name": "jacob_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
